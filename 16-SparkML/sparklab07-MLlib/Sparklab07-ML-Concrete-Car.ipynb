{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Lab 7: Using Spark MLlib for Feature Engineering and Prediction\n",
    "\n",
    "\n",
    "This lab focuses on building a ML pipeline with focus on feature data exploration and feature engineering. It has two parts:\n",
    "\n",
    "- Part 1 `Concrete Quality`: we focus on doing column statistics and engineering numerical features \n",
    "- Part 2 `Car Value`: we focus on engineering string columns.\n",
    "\n",
    "\n",
    "**Topics**: `describe, VectorAssembler, StandardScaler, Pipeline, StringIndexer, DecisionTreeClassifier, MulticlassClassificationEvaluator`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Tip**:If at any point you see this error, `AnalysisException: u'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;`. \n",
    "\n",
    "Please\n",
    "- removing the *.lck file from hive `metastore_db`\n",
    "```\n",
    "# assuming you're running this from your home directory from cloudera vm\n",
    "rm  metastore_db/*.lck\n",
    "```\n",
    "- Terminate all other running jupyter notebooks (from your jupyter home, go to Running tab, then terminate). \n",
    "\n",
    "If the above does not work still, try to restart the kernel (from your current jupyter notebook's menu, kernel > restart).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "1\\. Download and unzip the data files needed for this lab from \n",
    "\n",
    "[http://idsdl.csom.umn.edu/c/share/sparklab07data.zip](http://idsdl.csom.umn.edu/c/share/sparklab07data.zip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Prepare Concrete Quality Dataset for Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the lab uses adataset regarding the various properties and strength of concrete. Please complete the lab using `spark.ml` API functions. \n",
    "\n",
    "#|Field Name|Type| Description\n",
    "--|--|--|--\n",
    "0|cement|Double|Mass, in kg per cubic meter of mixture\n",
    "1|blast_furnace_slag|Double|Mass, in kg per cubic meter of mixture\n",
    "2|fly_ash|Double|Mass, in kg per cubic meter of mixture\n",
    "3|water|Double|Mass, in kg per cubic meter of mixture\n",
    "4|superplasticizer|Double|Mass, in kg per cubic meter of mixture\n",
    "5|course_aggregate|Double|Mass, in kg per cubic meter of mixture\n",
    "6|fine_aggregate|Double|Mass, in kg per cubic meter of mixture\n",
    "7|age|Double|Age, in days\n",
    "8|compressive_strength|Double|Strength, in megapascals (MPa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Sample the first few lines of `concrete_train.csv` using linux command(s), which helps you decide how to handle this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Load the `concrete_train.csv` file into a dataframe. Verify its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Because this dataset has numerical columns, it is useful to conduct summary statistics on it. Report the descriptive statistics\n",
    "\n",
    "**You may convert a DataFrame to pandas dataframe using `.toPandas()` for better readability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`SQLTransformer(statement=...)`](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.SQLTransformer) implements the transforms which are defined by SQL statement. \n",
    "\n",
    "Currently it only supports SQL syntax like `\"SELECT … FROM __THIS__\"` where `__THIS__` represents the underlying table of the input dataset.\n",
    "\n",
    "4\\. Define a SQLTransformer `st` that creates a new field `age_enc` that takes the following values:\n",
    "\n",
    "- 1, if age is between 0 and 30\n",
    "- 2, if age is > 30 and <= 90\n",
    "- 3, if age is > 90 and <= 180\n",
    "- 4, if age is 180 and above\n",
    "\n",
    "Click the above link to see the documentation/example of this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Use [`VectorAssembler`](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) to create a new `features` column with all fields except `compressive_strength` and `age`. \n",
    "\n",
    "If needed, click the link above to see an example from the official pyspark documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your assembler does what it needs to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler` transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. It takes parameters:\n",
    "\n",
    "- `withStd`: True by default. Scales the data to unit standard deviation.\n",
    "- `withMean`: False by default. Centers the data with mean before scaling. It will build a dense output, so take care when applying to sparse input.\n",
    "\n",
    "6\\. Create an instance of [`StandardScaler`](https://spark.apache.org/docs/latest/ml-features.html#standardscaler) called `ss`\n",
    "\n",
    "- it should apply to `features` and create a new column `scaledfeatures`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Create an instance of [`Pipeline`](https://spark.apache.org/docs/latest/ml-pipeline.html#example-pipeline) called `pl`\n",
    "\n",
    "-  Its `stages` should include the SQLTransformer, VectorAssembler and StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Use the `Pipeline` to transform the data and obtain a new dataframe, `transformed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect `scaledFeatures` and `features` column in the transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Using Decision Tree Classifiers to Predict Car Value\n",
    "\n",
    "During this exercise, you will build a Spark ML Pipeline to encode categorical string variables as integers before using them to build a model. \n",
    "\n",
    "The data used for this exercise concerns various properties of cars, and whether or not these cars were\n",
    "classified as a good value. The target value to be predictive is `acceptability`, which is a categorical variable representing whether or not a car is considered acceptable for purchase. All other feature variables are also **categorical**.\n",
    "\n",
    "#|Field|Data Type |Description\n",
    "--|--|--|--\n",
    "0|buying|String|Based on selling price\n",
    "1|maint|String|Based on cost to maintain the vehicle\n",
    "2|doors|String|Number of doors\n",
    "3|persons|String|Passenger capacity\n",
    "4|lug_boot|String|Based on luggage boot size\n",
    "5|safety|String|Based on estimated safety of the vehicle\n",
    "6|acceptability|String|Based on overall acceptability of the vehicle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Begin by importing the necessary modules for this exercise.\n",
    "\n",
    "If you’re using the Scala, you’ll use this code:\n",
    "```\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.feature.{StringIndexer,VectorAssembler}\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "```\n",
    "If you’re using the the PySpark, you’ll use this code:\n",
    "```\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1\\. Sample the first few lines of `cars_train.csv` using linux command(s), which helps you decide how to handle this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Load the `cars_train.csv` file into a DataFrame named `train_df`. Verify its schema & content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Explore the `buying`, `doors`,`persons`,`acceptability` columns by showing their distinct values. What kinds of columns are these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Create a new [`StringIndexer`](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer) for each of the columns, with the output column name in the form of `[colname]_ix` (for example, `buying` becomes `buying_ix`). Save these seven StringIndexers as `si1`, `si2`, `si3`, and so on.\n",
    "\n",
    "**Note**: the default sort order of `StringIndexer` is `frequencyDesc`, others include `frequencyAsc, alphabetDesc, alphabetAsc`. See the above link and click source to see more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Next, create a `VectorAssembler` called `va` to assemble each of the indexed columns **except `accetability_ix`** into a new column called `features`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Create a `DecisionTreeClassifier` \n",
    "\n",
    "- the label column should be `acceacceptability_ix` \n",
    "- the features column should be `features`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Create a new Spark ML `Pipeline` called `pl` \n",
    "\n",
    "- the `steps` should include all of the `StringIndexer`s,  the `VectorAssembler`, and the `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Create a PipelineModel named `plmodel` by fitting the pipeline on `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Create a new DataFrame called `test_df` from the `cars_test.csv` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. Applied the learned model on `test_df` and save the resultant DataFrame as `predictions`. \n",
    "\n",
    "- How many of the first 15 values in the `prediction` column match the values in the `acceptability_ix` column?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13\\. Using [`MulticlassClassificationEvaluator`](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator) to evaluate the predictions on the `accuracy` metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py3)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
