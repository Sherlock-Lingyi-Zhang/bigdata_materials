{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark Streaming\n",
    "\n",
    "_____\n",
    "### Note on  Streaming\n",
    "Streaming is something that is rapidly advancing and changin fast, there are multiple new libraries every year, new and different services always popping up, and what is in this notebook may or may not apply to you. Realistically speaking each situation is going to require a customized solution and there is not a one size fits all solution. Because of this, I wanted to point out some great resources for Python and Spark Streaming:\n",
    "\n",
    "* [The Official Documentation is great. This should be your first go to.](http://spark.apache.org/docs/latest/streaming-programming-guide.html#spark-streaming-programming-guide)\n",
    "\n",
    "* [Fantastic Guide to Spark Streaming with Kafka](https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/)\n",
    "\n",
    "* [Another Spark Streaming Example with Geo Plotting](http://nbviewer.jupyter.org/github/ibm-cds-labs/spark.samples/blob/master/notebook/DashDB%20Twitter%20Car%202015%20Python%20Notebook.ipynb)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites. Obtain Twitter API Credentials\n",
    "In order to use all of this though, we need to setup a Developer API acocunt with Twitter and create an application to get credentials. \n",
    "\n",
    "- make sure you have a twitter account\n",
    "- set up a Developer API account with Twitter\n",
    "- create an application to get credentials at [https://apps.twitter.com/](https://apps.twitter.com/)\n",
    "    + Consumer Key \n",
    "    + Consumer Secret \n",
    "    + Access Token\n",
    "    + Access Token Secret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. First let us install python **tweepy** package\n",
    "- this will be used by the tweetread.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the following first to ensure the course VM is temporarily connected to outside world (for installing tweepy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!export http_proxy=\"http://proxy.oit.umn.edu:3128\"\n",
    "!export https_proxy=\"https://proxy.oit.umn.edu:3128\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the tweepy installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure tweetread.py\n",
    "by copying and pasting the following from your Twitter API console\n",
    "+ Consumer Key \n",
    "+ Consumer Secret \n",
    "+ Access Token\n",
    "+ Access Token Secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run tweetread.py at a terminal using\n",
    "```bash\n",
    "python tweetread.py\n",
    "``` \n",
    "It will pause after saying \"Listening on port: 5555\". \n",
    "It will start receiving tweeter stream only after it accepts a connection on port 5555.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Verify spark context and sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# May cause deprecation warnings, safe to ignore, they aren't errors\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following ensures that we don't end up using multiple SQL context for different microbatches. Instead, we should maintain a single sqlContext instance for the given spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the function to obtain the a global SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. create streaming context and listen to socket 5555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. specify the sliding window for the Dstream\n",
    "\n",
    "- making the new window size 10 seconds and sliding interval 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Convert DStream RDDs into Hive temp tables\n",
    "\n",
    "- extract all hashtags from the tweet text\n",
    "- do a word count on hashtags\n",
    "- convert the result into a DataFrame\n",
    "- register the dataframe, sorted by count, as a temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Start the DStream \n",
    "- after this, you can no longer add more DStream operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. The driver monitors the temp table and visualize it\n",
    "\n",
    "we are using one simple technique of drawing a live figure in jupyter notebook\n",
    "https://github.com/anujgupta82/Musings/blob/master/Dynamic%20or%20Live%20update%20of%20a%20Plot.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))  # specify figure size in inches\n",
    "ax = fig.add_subplot(111)  # 1x1 grid\n",
    "plt.ion()  # turn on interacgive mode\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "for i in range(0,10):  # update 10 times then stop\n",
    "    ax.clear()  \n",
    "    # obtain the top 10 tags from tmp table and conver to pandas DF.\n",
    "    df_top_tags = get_sql_context_instance(sc).sql( 'Select tag, count from tweets order by count desc limit 10' ).toPandas()\n",
    "    y_pos = np.arange(df_top_tags.shape[0])\n",
    "    ax.barh(y_pos, df_top_tags['count'], align='center',color='green')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(df_top_tags['tag'])\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_title('Frequence of Live Hashtags')\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(1)  # sleep one 1 second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if something went wrong, you should use Step 9 and Step 10 to stop the streaming process and the tweeter data collection process respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9:  stop the spark streaming process \n",
    "- stop the streaming context but not the spark kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: stop the tweetread.py gracefully\n",
    "\n",
    "**pkill -f** allows you to kill a process by the command you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pkill -f 'python tweetread.py'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
