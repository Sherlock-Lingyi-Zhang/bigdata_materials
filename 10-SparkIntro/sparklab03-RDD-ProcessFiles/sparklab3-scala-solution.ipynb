{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Lab 3 - Process Data Files with Spark (Scala Solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you're working on the experimental VM, the default path is /vagrant/IPNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vagrant/IPNB\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrubbing - Fixing Issues in devicestatus.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-03-15:10:10:20,Sorrento F41L,8cc3b47e-bd01-4482-b500-28f2342679af,7,24,39,enabled,disabled,connected,55,67,12,33.6894754264,-117.543308253\n",
      "2014-03-15:10:10:20|MeeToo 1.0|ef8c7564-0a1a-4650-a655-c8bbd5f8f943|0|31|63|70|39|27|enabled|enabled|enabled|37.4321088904|-121.485029632\n",
      "2014-03-15:10:10:20|MeeToo 1.0|23eba027-b95a-4729-9a4b-a3cca51c5548|0|20|21|86|54|34|enabled|enabled|enabled|39.4378908349|-120.938978486\n",
      "2014-03-15:10:10:20,Sorrento F41L,707daba1-5640-4d60-a6d9-1d6fa0645be0,8,22,60,enabled,enabled,disabled,68,91,17,39.3635186767,-119.400334708\n",
      "2014-03-15:10:10:20,Ronin Novelty Note 1,db66fe81-aa55-43b4-9418-fc6e7a00f891,0,13,47,70,enabled,enabled,enabled,10,45,33.1913581092,-116.448242643\n",
      "2014-03-15:10:10:20,Sorrento F41L,ffa18088-69a0-433e-84b8-006b2b9cc1d0,3,10,36,enabled,connected,enabled,53,58,42,33.8343543748,-117.330000857\n",
      "2014-03-15:10:10:20,Sorrento F33L,66d678e6-9c87-48d2-a415-8d5035e54a23,1,34,74,enabled,disabled,enabled,57,42,15,37.3803954321,-121.840756755\n",
      "2014-03-15:10:10:20|MeeToo 4.1|673f7e4b-d52b-44fc-8826-aea460c3481a|1|16|77|61|24|50|enabled|disabled|enabled|34.1841062345|-117.9435329\n",
      "2014-03-15:10:10:20,Ronin Novelty Note 2,a678ccc3-b0d2-452d-bf89-85bd095e28ee,0,10,97,63,enabled,enabled,connected,48,4,32.2850556785,-111.819583734\n",
      "2014-03-15:10:10:20,Sorrento F41L,86bef6ae-2f1c-42ec-aa67-6acecd7b0675,9,27,35,enabled,enabled,enabled,62,41,19,45.2400522984,-122.377467861\n"
     ]
    }
   ],
   "source": [
    "!head dev1data/devicestatus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Load the file from local path\n",
    "When you load data in spark, you must use the full path. pyspark cannot recognize variables such as $DEV1DATA (which is defined in local bash shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mydata: org.apache.spark.rdd.RDD[String] = file:/vagrant/IPNB/dev1data/devicestatus.txt MapPartitionsRDD[11] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mydata = sc.textFile(\"file:/vagrant/IPNB/dev1data/devicestatus.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.\tDetermine which delimiter to use \n",
    "**hint**: the character at position 19 is the first use of the delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[Char] = Array(|, ,, /)\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.map(_(19)).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.\tFilter out any records which do not parse correctly \n",
    "**hint**: each record should have exactly 14 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parsed: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[20] at map at <console>:27\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsed  = mydata.map(line=>line.split(line(19)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Long = 459540\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created an RDD that removes the rows that did not parse correctly (i.e. not containing 14 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtered: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[21] at filter at <console>:29\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filtered = parsed.filter(_.length==14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare the # of records two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Long = 459540\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\tExtract the fields\n",
    "Extra date (first field), model (second field), device ID (third field), and latitude and longitude (13th and 14th fields respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectedFields: org.apache.spark.rdd.RDD[(String, String, String, String, String)] = MapPartitionsRDD[23] at map at <console>:31\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val selectedFields = filtered.map(r=>(r(0),r(1),r(2),r(12),r(13)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspect the results by printing first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-03-15:10:10:20, Sorrento F41L, 8cc3b47e-bd01-4482-b500-28f2342679af, 33.6894754264, -117.543308253\n",
      "2014-03-15:10:10:20, MeeToo 1.0, ef8c7564-0a1a-4650-a655-c8bbd5f8f943, 37.4321088904, -121.485029632\n",
      "2014-03-15:10:10:20, MeeToo 1.0, 23eba027-b95a-4729-9a4b-a3cca51c5548, 39.4378908349, -120.938978486\n",
      "2014-03-15:10:10:20, Sorrento F41L, 707daba1-5640-4d60-a6d9-1d6fa0645be0, 39.3635186767, -119.400334708\n",
      "2014-03-15:10:10:20, Ronin Novelty Note 1, db66fe81-aa55-43b4-9418-fc6e7a00f891, 33.1913581092, -116.448242643\n",
      "2014-03-15:10:10:20, Sorrento F41L, ffa18088-69a0-433e-84b8-006b2b9cc1d0, 33.8343543748, -117.330000857\n",
      "2014-03-15:10:10:20, Sorrento F33L, 66d678e6-9c87-48d2-a415-8d5035e54a23, 37.3803954321, -121.840756755\n",
      "2014-03-15:10:10:20, MeeToo 4.1, 673f7e4b-d52b-44fc-8826-aea460c3481a, 34.1841062345, -117.9435329\n",
      "2014-03-15:10:10:20, Ronin Novelty Note 2, a678ccc3-b0d2-452d-bf89-85bd095e28ee, 32.2850556785, -111.819583734\n",
      "2014-03-15:10:10:20, Sorrento F41L, 86bef6ae-2f1c-42ec-aa67-6acecd7b0675, 45.2400522984, -122.377467861\n"
     ]
    }
   ],
   "source": [
    "selectedFields.take(10).foreach(row=>println(s\"${row._1}, ${row._2}, ${row._3}, ${row._4}, ${row._5}\"))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.\t Split the 2nd field into two\n",
    "The second field contains the device manufacturer and model name (e.g. Ronin S2.) Split this field by spaces to separate the manufacturer from the model (e.g. manufacturer Ronin, model S2.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splitted: org.apache.spark.rdd.RDD[(String, String, String, String, String, String)] = MapPartitionsRDD[24] at map at <console>:33\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splitted = selectedFields.map(r=>(r._1,r._2.split(' ')(0),r._2.split(' ')(1),r._3,r._4,r._5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.  Save CSV to HDFS\n",
    "Save the extracted data to comma delimited text files in the **/loudacre/devicestatus_etl** directory on HDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.FileAlreadyExistsException",
     "evalue": " Output directory file:/vagrant/IPNB/devicestatus_etl already exists",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/vagrant/IPNB/devicestatus_etl already exists",
      "  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)",
      "  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:283)",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)",
      "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1493)",
      "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)",
      "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1472)",
      "  ... 36 elided",
      ""
     ]
    }
   ],
   "source": [
    "// .productIterator.toList converts tuple to List.\n",
    "// .mkString implodes string with delimiter\n",
    "splitted.map(_.productIterator.toList.mkString(\",\")).saveAsTextFile(\"file:/vagrant/IPNB/devicestatus_etl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.  Verify Results\n",
    "verify results with HDFS commands\n",
    "- first to show content of **/loudacre/devicestatus_etl** \n",
    "- then take sample rows from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 43517\n",
      "-rw-rw-r-- 1 vagrant vagrant 22281863 Nov  7 00:11 part-00000\n",
      "-rw-rw-r-- 1 vagrant vagrant 22278372 Nov  7 00:11 part-00001\n",
      "-rw-rw-r-- 1 vagrant vagrant        0 Nov  7 00:11 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls -l /vagrant/IPNB/devicestatus_etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-03-15:10:10:20,Sorrento,F41L,8cc3b47e-bd01-4482-b500-28f2342679af,33.6894754264,-117.543308253\n",
      "2014-03-15:10:10:20,MeeToo,1.0,ef8c7564-0a1a-4650-a655-c8bbd5f8f943,37.4321088904,-121.485029632\n",
      "2014-03-15:10:10:20,MeeToo,1.0,23eba027-b95a-4729-9a4b-a3cca51c5548,39.4378908349,-120.938978486\n",
      "2014-03-15:10:10:20,Sorrento,F41L,707daba1-5640-4d60-a6d9-1d6fa0645be0,39.3635186767,-119.400334708\n",
      "2014-03-15:10:10:20,Ronin,Novelty,db66fe81-aa55-43b4-9418-fc6e7a00f891,33.1913581092,-116.448242643\n",
      "2014-03-15:10:10:20,Sorrento,F41L,ffa18088-69a0-433e-84b8-006b2b9cc1d0,33.8343543748,-117.330000857\n",
      "2014-03-15:10:10:20,Sorrento,F33L,66d678e6-9c87-48d2-a415-8d5035e54a23,37.3803954321,-121.840756755\n",
      "2014-03-15:10:10:20,MeeToo,4.1,673f7e4b-d52b-44fc-8826-aea460c3481a,34.1841062345,-117.9435329\n",
      "2014-03-15:10:10:20,Ronin,Novelty,a678ccc3-b0d2-452d-bf89-85bd095e28ee,32.2850556785,-111.819583734\n",
      "2014-03-15:10:10:20,Sorrento,F41L,86bef6ae-2f1c-42ec-aa67-6acecd7b0675,45.2400522984,-122.377467861\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!cat /vagrant/IPNB/devicestatus_etl/* | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 (SPylon)",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
