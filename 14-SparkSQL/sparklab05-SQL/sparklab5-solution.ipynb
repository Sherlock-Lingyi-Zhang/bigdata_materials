{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Lab 5 - Using Spark SQL (Solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we practice the critical skills of creating a proper data frame from a source dataset (CSV), querying it using both Spark.sql and DataFrame APIs, and save results to the storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the `auctiondata.csv` distributed with `sparkdata.zip`. If you have not previously downloaded sparkdata.zip, you can download it from `http://idsdl.csom.umn.edu/c/share/sparkdata.zip` using `wget`. Alternatively, you can copy the URL in your browser and download it from there. \n",
    "\n",
    "For this lab, you can do it using cloudera VM (on HDFS) or using any of the alternative spark environment (using a local copy of the file).\n",
    "\n",
    "Our dataset is a `.csv` file that consists of online auction data. Each auction has an auction id associated with it and can have multiple bids. Each row represents a bid. For each bid, we have the following information:\n",
    "\n",
    "Column|Type| Description\n",
    "--|--|--\n",
    "`aucid`|String| Auction ID\n",
    "`bid`|Float| Bid amount\n",
    "`bidtime`|Float| Time of bid from start of auction\n",
    "`bidder`|String| The bidder’s userid\n",
    "`bidrate`|Int| The bidder’s rating\n",
    "`openbid`|Float| Opening price\n",
    "`Price`|Float| Final price\n",
    "`itemtype`|String| Item type\n",
    "`dtl`|Int| Days to live\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Explore the data first from shell.\n",
    "What does the data looks like? Does it have a header row? What is the delimiter?\n",
    "\n",
    "**tip**: you can run operating system command in notebook cells by prefixing your command with \"!\". e.g., \n",
    "```bash\n",
    "! ls\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8213034705,95,2.927373,jake7870,0,95,117.5,xbox,3\r\n",
      "8213034705,115,2.943484,davidbresler2,1,95,117.5,xbox,3\r\n",
      "8213034705,100,2.951285,gladimacowgirl,58,95,117.5,xbox,3\r\n",
      "8213034705,117.5,2.998947,daysrus,10,95,117.5,xbox,3\r\n",
      "8213060420,2,0.065266,donnie4814,5,1,120,xbox,3\r\n",
      "8213060420,15.25,0.123218,myreeceyboy,52,1,120,xbox,3\r\n",
      "8213060420,3,0.186539,parakeet2004,5,1,120,xbox,3\r\n",
      "8213060420,10,0.18669,parakeet2004,5,1,120,xbox,3\r\n",
      "8213060420,24.99,0.187049,parakeet2004,5,1,120,xbox,3\r\n",
      "8213060420,20,0.249491,bluebubbles_1,25,1,120,xbox,3\r\n"
     ]
    }
   ],
   "source": [
    "!head sparkdata/auctiondata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.Creating the DataFrame\n",
    "\n",
    "To there are several ways to create a new dataframe \n",
    "\n",
    "### Approach 1: The RDD route\n",
    "\n",
    "Read the text file into an RDD, convert the RDD to RDD[Row] with proper field names and data types, then convert the RDD to a DataFrame. \n",
    "\n",
    "This approach is useful if you are dealing with raw data (maybe unstructured) and you want to explore it before turning it into a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"sparkdata/auctiondata.csv\"\n",
    "rawDataRDD = sc.textFile(data_file).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "csvRDD = rawDataRDD.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# convert\n",
    "rowRDD = csvRDD.map(lambda p: Row(\n",
    "    auctionid=p[0], \n",
    "    bid=float(p[1]),\n",
    "    bidtime=float(p[2]),\n",
    "    bidder=p[3],\n",
    "    bidrate=int(p[4]),\n",
    "    openbid=float(p[5]),\n",
    "    price=float(p[6]),\n",
    "    itemtype=p[7],\n",
    "    dtl=int(p[8]),\n",
    "    )\n",
    ")\n",
    "\n",
    "bids = spark.createDataFrame(rowRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the column names and data types of the dataFrame. \n",
    "\n",
    "**Question**: Are the data types the same as as what you have specified. What do you think has happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- auctionid: long (nullable = true)\n",
      " |-- bid: double (nullable = true)\n",
      " |-- bidder: string (nullable = true)\n",
      " |-- bidrate: long (nullable = true)\n",
      " |-- bidtime: double (nullable = true)\n",
      " |-- dtl: long (nullable = true)\n",
      " |-- itemtype: string (nullable = true)\n",
      " |-- openbid: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bidsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The data types are not the same. Some are converted from string --> long, or float-->double. Spark reader has internal mechanisms to convert input data types into types that are best suited for Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the data frame by showing its first 5 rows in a tablular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------+-------+--------+---+--------+-------+-----+\n",
      "| auctionid|  bid|        bidder|bidrate| bidtime|dtl|itemtype|openbid|price|\n",
      "+----------+-----+--------------+-------+--------+---+--------+-------+-----+\n",
      "|8213034705| 95.0|      jake7870|      0|2.927373|  3|    xbox|   95.0|117.5|\n",
      "|8213034705|115.0| davidbresler2|      1|2.943484|  3|    xbox|   95.0|117.5|\n",
      "|8213034705|100.0|gladimacowgirl|     58|2.951285|  3|    xbox|   95.0|117.5|\n",
      "|8213034705|117.5|       daysrus|     10|2.998947|  3|    xbox|   95.0|117.5|\n",
      "|8213060420|  2.0|    donnie4814|      5|0.065266|  3|    xbox|    1.0|120.0|\n",
      "+----------+-----+--------------+-------+--------+---+--------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bidsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: using DataFrame Reader for CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the CSV reader to read the file, and try to infer schema from the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids = spark.read.option(\"inferSchema\",\"true\").csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the schema, you'll notice the columns have automatic column names such as \"_c0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: long (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: double (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bids.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach a schema (string) at the loading time.\n",
    "One way you can add a schema (col names) is to use the `schema(schema_str)` option of DataFrame Reader, where schema_str takes the form of `\"col1 INT, col2 String\"`. For your convenience, we have provided that column string below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_str = \"\"\"\n",
    "    auctionid long, bid double, bidtime double, bidder string,\n",
    "    bidrate long, openbid double, price double,  \n",
    "    itemtype string, dtl long\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recreate the dataframe `bids` using the `schema_str` and verify the schema of the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids = spark.read.schema(schema_str).format(\"csv\").load(\"sparkdata/auctiondata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- auctionid: long (nullable = true)\n",
      " |-- bid: double (nullable = true)\n",
      " |-- bidder: string (nullable = true)\n",
      " |-- bidrate: long (nullable = true)\n",
      " |-- bidtime: double (nullable = true)\n",
      " |-- dtl: long (nullable = true)\n",
      " |-- itemtype: string (nullable = true)\n",
      " |-- openbid: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bids.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach a schema (StructType) at the loading time.\n",
    "\n",
    "The `schema()` can also take a StructType as argument. In our lecture, we have shown you an example of creating a StructType schema.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([    \n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is your turn to create the StructType for our dataset and use it when you load a DataFrame from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#define a structtype as schema\n",
    "schema_structtype = StructType([\n",
    " StructField(\"auctionid\",LongType(),True),\n",
    " StructField(\"bid\",DoubleType(),True),\n",
    " StructField(\"bidtime\",DoubleType(),True),\n",
    " StructField(\"bidder\",StringType(),True),\n",
    " StructField(\"bidrate\",LongType(),True),\n",
    " StructField(\"openbid\",DoubleType(),True),\n",
    " StructField(\"price\",DoubleType(),True),\n",
    " StructField(\"itemtype\",StringType(),True),\n",
    " StructField(\"dtl\",LongType(),True)\n",
    "])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids = spark.read.schema(schema_structtype).csv(\"sparkdata/auctiondata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your dataframe's schema and top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- auctionid: long (nullable = true)\n",
      " |-- bid: double (nullable = true)\n",
      " |-- bidtime: double (nullable = true)\n",
      " |-- bidder: string (nullable = true)\n",
      " |-- bidrate: long (nullable = true)\n",
      " |-- openbid: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- itemtype: string (nullable = true)\n",
      " |-- dtl: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bids.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+--------------+-------+-------+-----+--------+---+\n",
      "| auctionid|  bid| bidtime|        bidder|bidrate|openbid|price|itemtype|dtl|\n",
      "+----------+-----+--------+--------------+-------+-------+-----+--------+---+\n",
      "|8213034705| 95.0|2.927373|      jake7870|      0|   95.0|117.5|    xbox|  3|\n",
      "|8213034705|115.0|2.943484| davidbresler2|      1|   95.0|117.5|    xbox|  3|\n",
      "|8213034705|100.0|2.951285|gladimacowgirl|     58|   95.0|117.5|    xbox|  3|\n",
      "|8213034705|117.5|2.998947|       daysrus|     10|   95.0|117.5|    xbox|  3|\n",
      "|8213060420|  2.0|0.065266|    donnie4814|      5|    1.0|120.0|    xbox|  3|\n",
      "+----------+-----+--------+--------------+-------+-------+-----+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bids.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add column names after you create the schema\n",
    "\n",
    "Another way is to rename column names after you have created the dataframe. \n",
    "\n",
    "You can do it using `withColumnRenamed` but that is tedious. \n",
    "\n",
    "Alternatively, you can use the `toDF(*colnames)` function to create a new dataframe with given names, where `*colnames` is a list of names, e.g. `toDF(\"name\", \"age\")`.\n",
    "\n",
    "For your convenience, we have provided you a list of names in `cols`, please use this to rename columns after you load the data (recreate the \"unnamed\" DataFrame if you have overwritten it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    " 'auctionid',\n",
    " 'bid',\n",
    " 'bidtime',\n",
    " 'bidder',\n",
    " 'bidrate',\n",
    " 'openbid',\n",
    " 'price',\n",
    " 'itemtype',\n",
    " 'dtl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids = spark.read.option(\"inferSchema\",True).csv(\"sparkdata/auctiondata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids = bids.toDF(*cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your dataframe's schema and top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- auctionid: long (nullable = true)\n",
      " |-- bid: double (nullable = true)\n",
      " |-- bidtime: double (nullable = true)\n",
      " |-- bidder: string (nullable = true)\n",
      " |-- bidrate: integer (nullable = true)\n",
      " |-- openbid: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- itemtype: string (nullable = true)\n",
      " |-- dtl: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bids.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+--------------+-------+-------+-----+--------+---+\n",
      "| auctionid|  bid| bidtime|        bidder|bidrate|openbid|price|itemtype|dtl|\n",
      "+----------+-----+--------+--------------+-------+-------+-----+--------+---+\n",
      "|8213034705| 95.0|2.927373|      jake7870|      0|   95.0|117.5|    xbox|  3|\n",
      "|8213034705|115.0|2.943484| davidbresler2|      1|   95.0|117.5|    xbox|  3|\n",
      "|8213034705|100.0|2.951285|gladimacowgirl|     58|   95.0|117.5|    xbox|  3|\n",
      "|8213034705|117.5|2.998947|       daysrus|     10|   95.0|117.5|    xbox|  3|\n",
      "|8213060420|  2.0|0.065266|    donnie4814|      5|    1.0|120.0|    xbox|  3|\n",
      "+----------+-----+--------+--------------+-------+-------+-----+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bids.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Queries on your DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 1**: We are interested in auction id `1645914432`, and want to show a list of bids by descending  order of bid times. show `bid`, `bidder`, and `bidtime`. Implement this using the DataFrame API approach (i.e. using DataFrame's methods such as .select, .filter etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+\n",
      "|  bid|     bidder| bidtime|\n",
      "+-----+-----------+--------+\n",
      "|511.0|gracedivine|2.645231|\n",
      "|501.0|   beelprez|2.379583|\n",
      "|451.0|   beelprez|2.379352|\n",
      "|405.0|   beelprez| 2.37912|\n",
      "|310.0|   beelprez|2.378866|\n",
      "|280.0|   beelprez|2.374896|\n",
      "|260.0|   beelprez|2.374711|\n",
      "|250.0|   beelprez|2.374502|\n",
      "|500.0| jcobb74787|2.029803|\n",
      "|220.0|   beelprez|1.943183|\n",
      "|210.0|    leakang|0.882593|\n",
      "|200.0|   beelprez|0.433669|\n",
      "+-----+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bids.filter(bids.auctionid==1645914432) \\\n",
    "    .select(\"bid\",\"bidder\",\"bidtime\").sort(bids.bidtime.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 2**: Fetch the maximum price (col name: `max_price`) and number of bids (col name `num_bids`) by item type (`itemtype`) using the spark.SQL approach (note that you need to create view first). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.createOrReplaceTempView(\"bids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemtypes = spark.sql(\"\"\"\n",
    "    SELECT itemtype, max(price) as max_price, count(*) as num_bids \n",
    "    from bids \n",
    "    group by itemtype\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+\n",
      "|itemtype|max_price|num_bids|\n",
      "+--------+---------+--------+\n",
      "| cartier|   5400.0|    1953|\n",
      "|    palm|    290.0|    5917|\n",
      "|    xbox|   501.77|    2784|\n",
      "+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemtypes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 3**: Complete the query 2 using the DataFrame API approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+\n",
      "|itemtype|max_price|num_bids|\n",
      "+--------+---------+--------+\n",
      "| cartier|   5400.0|    1953|\n",
      "|    palm|    290.0|    5917|\n",
      "|    xbox|   501.77|    2784|\n",
      "+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "# the count function can count any columns\n",
    "bids.groupBy(\"itemtype\").agg(f.max(bids.price).alias(\"max_price\"),f.count(bids.price).alias(\"num_bids\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 4**: For all bids on \"cartier\", find out the maximum bid price for all associated auctions, display auctionid, itemtype, and price. Using the API approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxprices = bids.select(\"auctionid\",\"itemtype\",\"price\") \\\n",
    "    .filter(bids.itemtype=='cartier') \\\n",
    "    .groupBy(\"auctionid\") \\\n",
    "    .max(\"price\") \\\n",
    "    .withColumnRenamed(\"max(price)\",\"max_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "| auctionid|max_price|\n",
      "+----------+---------+\n",
      "|1642534283|   1125.0|\n",
      "|1645542737|    386.0|\n",
      "|1644077790|   1400.0|\n",
      "|1644724061|    305.0|\n",
      "|1647149304|   752.56|\n",
      "|1640495398|   1800.0|\n",
      "|1644109746|   3103.0|\n",
      "|1647329406|   1000.0|\n",
      "|1644752795|   1800.0|\n",
      "|1642875447|    831.0|\n",
      "|1644197869|    300.0|\n",
      "|1647320738|   452.87|\n",
      "|1649718196|   1799.0|\n",
      "|1640793161|   2395.0|\n",
      "|1643244227|   1025.0|\n",
      "|1644343468|  1038.99|\n",
      "|1642561397|   1825.0|\n",
      "|1646448593|    202.5|\n",
      "|1646988233|    620.0|\n",
      "|1645989170|    280.0|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxprices.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Write the results in CSV\n",
    "You can use Spark Dataframe Writer to write results to desirable formats. Here we ask you to write the results in csv files. Keep in mind that in big data, our dataset = folders. \n",
    "\n",
    "Save the result of your previous query (dataframe) in a folder called \"maxprices\" in the csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxprices.write.csv(\"maxprices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the files in the folder. **Question**: why there are multiple files? Which one/ones are your result set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 51\r\n",
      "-rw-rw-r-- 1 vagrant vagrant  0 Jul 31 15:59 part-00000-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00001-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 35 Jul 31 15:59 part-00002-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00004-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00006-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 36 Jul 31 15:59 part-00007-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00010-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00012-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 70 Jul 31 15:59 part-00013-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00014-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 37 Jul 31 15:59 part-00017-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00023-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00028-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00029-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00031-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 36 Jul 31 15:59 part-00033-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 34 Jul 31 15:59 part-00036-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 51 Jul 31 15:59 part-00041-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00042-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00043-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00044-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 34 Jul 31 15:59 part-00045-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00046-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00048-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00051-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 36 Jul 31 15:59 part-00052-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00053-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00055-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00057-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00058-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00060-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00061-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00064-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 53 Jul 31 15:59 part-00068-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00070-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00074-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00075-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00076-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 35 Jul 31 15:59 part-00077-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00080-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00081-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00082-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 35 Jul 31 15:59 part-00083-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00084-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00086-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00087-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00088-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00089-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00092-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00094-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00096-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00097-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 35 Jul 31 15:59 part-00098-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00099-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00100-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00102-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00104-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 34 Jul 31 15:59 part-00105-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 36 Jul 31 15:59 part-00106-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00111-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00112-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00113-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 35 Jul 31 15:59 part-00115-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00116-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00117-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00120-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00122-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 34 Jul 31 15:59 part-00123-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00125-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00126-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00127-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00129-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00132-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00136-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00138-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 35 Jul 31 15:59 part-00140-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00141-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 36 Jul 31 15:59 part-00142-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00143-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00144-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00145-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 52 Jul 31 15:59 part-00146-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 34 Jul 31 15:59 part-00148-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00149-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00150-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 36 Jul 31 15:59 part-00152-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00156-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00161-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00164-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00165-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00166-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00170-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00171-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 35 Jul 31 15:59 part-00172-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 86 Jul 31 15:59 part-00174-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 36 Jul 31 15:59 part-00175-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00182-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00183-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 18 Jul 31 15:59 part-00189-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 35 Jul 31 15:59 part-00191-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00196-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 17 Jul 31 15:59 part-00198-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant 16 Jul 31 15:59 part-00199-ca1de9b7-b178-4376-85a3-e6dd948f0723-c000.csv\r\n",
      "-rw-rw-r-- 1 vagrant vagrant  0 Jul 31 15:59 _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l maxprices/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: all of the csv files are part of the result. You have multiple files because of parallel processing (each partition of your data may write its own output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take your data out in a single file.\n",
    "\n",
    "*Hint: consider using Linux rediretion (\">\") to create this single file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat maxprices/* > maxprices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1642534283,1125.0\r\n",
      "1645542737,386.0\r\n",
      "1644077790,1400.0\r\n",
      "1644724061,305.0\r\n",
      "1647149304,752.56\r\n",
      "1640495398,1800.0\r\n",
      "1644109746,3103.0\r\n",
      "1647329406,1000.0\r\n",
      "1644752795,1800.0\r\n",
      "1642875447,831.0\r\n"
     ]
    }
   ],
   "source": [
    "! head maxprices.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py3)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
