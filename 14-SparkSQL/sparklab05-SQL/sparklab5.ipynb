{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Lab 5 - Using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we practice the critical skills of creating a proper data frame from a source dataset (CSV), querying it using both Spark.sql and DataFrame APIs, and save results to the storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the `auctiondata.csv` distributed with `sparkdata.zip`. If you have not previously downloaded sparkdata.zip, you can download it from `http://idsdl.csom.umn.edu/c/share/sparkdata.zip` using `wget`. Alternatively, you can copy the URL in your browser and download it from there. \n",
    "\n",
    "For this lab, you can do it using cloudera VM (on HDFS) or using any of the alternative spark environment (using a local copy of the file).\n",
    "\n",
    "Our dataset is a `.csv` file that consists of online auction data. Each auction has an auction id associated with it and can have multiple bids. Each row represents a bid. For each bid, we have the following information:\n",
    "\n",
    "Column|Type| Description\n",
    "--|--|--\n",
    "`aucid`|String| Auction ID\n",
    "`bid`|Float| Bid amount\n",
    "`bidtime`|Float| Time of bid from start of auction\n",
    "`bidder`|String| The bidder’s userid\n",
    "`bidrate`|Int| The bidder’s rating\n",
    "`openbid`|Float| Opening price\n",
    "`Price`|Float| Final price\n",
    "`itemtype`|String| Item type\n",
    "`dtl`|Int| Days to live\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Explore the data first from shell.\n",
    "What does the data looks like? Does it have a header row? What is the delimiter?\n",
    "\n",
    "**tip**: you can run operating system command in notebook cells by prefixing your command with \"!\". e.g., \n",
    "```bash\n",
    "! ls\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.Creating the DataFrame\n",
    "\n",
    "To there are several ways to create a new dataframe \n",
    "\n",
    "### Approach 1: The RDD route\n",
    "\n",
    "Read the text file into an RDD, convert the RDD to RDD[Row] with proper field names and data types, then convert the RDD to a DataFrame. \n",
    "\n",
    "This approach is useful if you are dealing with raw data (maybe unstructured) and you want to explore it before turning it into a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the column names and data types of the dataFrame. \n",
    "\n",
    "**Question**: Are the data types the same as as what you have specified. What do you think has happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The data types are not the same. Some are converted from string --> long, or float-->double. Spark reader has internal mechanisms to convert input data types into types that are best suited for Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the data frame by showing its first 5 rows in a tablular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: using DataFrame Reader for CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the CSV reader to read the file, and try to infer schema from the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the schema, you'll notice the columns have automatic column names such as \"_c0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach a schema (string) at the loading time.\n",
    "One way you can add a schema (col names) is to use the `schema(schema_str)` option of DataFrame Reader, where schema_str takes the form of `\"col1 INT, col2 String\"`. For your convenience, we have provided that column string below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_str = \"\"\"\n",
    "    auctionid long, bid double, bidtime double, bidder string,\n",
    "    bidrate long, openbid double, price double,  \n",
    "    itemtype string, dtl long\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recreate the dataframe `bids` using the `schema_str` and verify the schema of the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach a schema (StructType) at the loading time.\n",
    "\n",
    "The `schema()` can also take a StructType as argument. In our lecture, we have shown you an example of creating a StructType schema.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([    \n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is your turn to create the StructType for our dataset and use it when you load a DataFrame from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your dataframe's schema and top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add column names after you create the schema\n",
    "\n",
    "Another way is to rename column names after you have created the dataframe. \n",
    "\n",
    "You can do it using `withColumnRenamed` but that is tedious. \n",
    "\n",
    "Alternatively, you can use the `toDF(*colnames)` function to create a new dataframe with given names, where `*colnames` is a list of names, e.g. `toDF(\"name\", \"age\")`.\n",
    "\n",
    "For your convenience, we have provided you a list of names in `cols`, please use this to rename columns after you load the data (recreate the \"unnamed\" DataFrame if you have overwritten it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    " 'auctionid',\n",
    " 'bid',\n",
    " 'bidtime',\n",
    " 'bidder',\n",
    " 'bidrate',\n",
    " 'openbid',\n",
    " 'price',\n",
    " 'itemtype',\n",
    " 'dtl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your dataframe's schema and top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Queries on your DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 1**: We are interested in auction id `1645914432`, and want to show a list of bids by descending  order of bid times. show `bid`, `bidder`, and `bidtime`. Implement this using the DataFrame API approach (i.e. using DataFrame's methods such as .select, .filter etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 2**: Fetch the maximum price (col name: `max_price`) and number of bids (col name `num_bids`) by item type (`itemtype`) using the spark.SQL approach (note that you need to create view first). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 3**: Complete the query 2 using the DataFrame API approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query 4**: For all bids on \"cartier\", find out the maximum bid price for all associated auctions, display auctionid, itemtype, and price. Using the API approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Write the results in CSV\n",
    "You can use Spark Dataframe Writer to write results to desirable formats. Here we ask you to write the results in csv files. Keep in mind that in big data, our dataset = folders. \n",
    "\n",
    "Save the result of your previous query (dataframe) in a folder called \"maxprices\" in the csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the files in the folder. **Question**: why there are multiple files? Which one/ones are your result set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: all of the csv files are part of the result. You have multiple files because of parallel processing (each partition of your data may write its own output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take your data out in a single file.\n",
    "\n",
    "*Hint: consider using Linux rediretion (\">\") to create this single file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py3)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
