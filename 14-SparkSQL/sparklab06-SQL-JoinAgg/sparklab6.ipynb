{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Lab 6: Use Spark SQL to Join and Aggregate Data\n",
    "\n",
    "In this lab, we will learn a few things:\n",
    "\n",
    "- how to read json type (a specific kind with each row is a json-encoded record) data into Spark SQL\n",
    "- how to use SQL functions to do some more advanced data manipulations (e.g. split).\n",
    "- how to do Inner joins \n",
    "- how to do some reporting using windowing functions (more advanced, but optional).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use a few files distributed with `sparkdata.zip`. If you have not previously downloaded `sparkdata.zip`, you can download it from `http://idsdl.csom.umn.edu/c/share/sparkdata.zip` using `wget`. Alternatively, you can copy the URL in your browser and download it from there. \n",
    "\n",
    "- `loudacre/device.json`: list of devices\n",
    "- `loudacre/webpage.json`: inventory of webpages\n",
    "- `loudacre/websitehit.json`:  hits on webpage with device_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Inspect the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you want to inspect the data so that you understand its format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OS commands to view a sample of each json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: \n",
    "- given the format, what is the best way to read these files?  \n",
    "- Do we have consistent field names across tables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Read, Inspect, and Analyze `webpage`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load `webpage.json` into a DataFrame called `webpage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the schema and first 20 rows. Fix any issue, if any, before you proceed.\n",
    "\n",
    "**tip**: use `show(truncate=False)` to show long fields completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You notice that the associated_files lists multiple files separated by commas. Next we want to list these files individually, such as:\n",
    "\n",
    "```\n",
    "+------------+-----------------+\n",
    "|web_page_num|       assoc_file|\n",
    "+------------+-----------------+\n",
    "|           1|        theme.css|\n",
    "|           1|          code.js|\n",
    "|           1|sorrento_f00l.jpg|\n",
    "|           2|        theme.css|\n",
    "|           2|          code.js|\n",
    "|           2| titanic_2100.jpg|\n",
    "```\n",
    "\n",
    "Achive the above goal (this helps, for example, you to run query on file hits).\n",
    "\n",
    "- i.e. create a dataframe `page_files` with `web_page_num` and `assoc_file`\n",
    "\n",
    "**Hint**: consider using Spark SQL functions to first split the field, then explode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify what you obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To practice JOIN with Spark, we ask you to join the `webpage` and `page_files`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Find top most-used devices for each page (optional, more challenging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a user visits a page using a device, this gets saved to `websitehit`. We want to analyze **for each webpage, what are the top 2 devices used for visiting this page**? \n",
    "\n",
    "This is most conveniently accomplished using Spark SQL's window functions (in particular its `rank()` function, because if you can get the rank of records by # of hits per device, then you can filter the dataset by rank to show just the first two). if you need refresher of window functions, you can visit [https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html).\n",
    "\n",
    "\n",
    "But before you can get the top ranks, you may first need to aggregate the data to get the # of hits.\n",
    "\n",
    "In the end, we want you to show something like this (some table joins are needed, think about when you should do joins):\n",
    "\n",
    "```\n",
    "+-------------------------------+-------------+----+\n",
    "|web_page_file_name             |device_name  |hits|\n",
    "+-------------------------------+-------------+----+\n",
    "|sorrento_f30l_sales.html       |Sorrento F41L|125 |\n",
    "|sorrento_f30l_sales.html       |Titanic 1100 |68  |\n",
    "|sorrento_f41l_sales.html       |Sorrento F41L|116 |\n",
    "|sorrento_f41l_sales.html       |Titanic 1000 |64  |\n",
    "|ronin_novelty_note_4_sales.html|Sorrento F41L|123 |\n",
    "|ronin_novelty_note_4_sales.html|Titanic 1100 |63  |\n",
    "|sorrento_f24l_sales.html       |Sorrento F41L|122 |\n",
    "|sorrento_f24l_sales.html       |Titanic 1100 |63  |\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py3)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
